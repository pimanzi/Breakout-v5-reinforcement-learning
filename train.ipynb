{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pimanzi/Breakout-v5-reinforcement-learning/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542a382d",
      "metadata": {
        "id": "542a382d"
      },
      "source": [
        "# üéÆ Training a DQN Agent for Breakout-v5\n",
        "This notebook provides a clean and modular training pipeline for Deep Q-Learning using **Breakout-v5**.\n",
        "You may **ONLY modify hyperparameters** in the marked section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aacc912",
      "metadata": {
        "id": "3aacc912"
      },
      "source": [
        "## üì¶ Step 1: Install and Import Dependencies\n",
        "\n",
        "**What happens here:**\n",
        "We install all the necessary Python libraries that our DQN agent needs to work. Think of these as tools in a toolbox - each one has a specific job to do in training our AI to play Breakout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36e0bbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a36e0bbe",
        "outputId": "c19407dd-f996-4fa3-9149-b7b1b75b306e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n",
            "‚úÖ All packages installed successfully!\n",
            "‚úÖ Found 104 ALE/Atari environments\n",
            "üéâ You're ready to go! Continue to the next cells.\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install -q gymnasium ale-py opencv-python numpy matplotlib tensorflow\n",
        "\n",
        "# Install AutoROM and download ROMs\n",
        "!pip install -q autorom\n",
        "!AutoROM --accept-license\n",
        "\n",
        "# Import ale_py to automatically register ROMs\n",
        "import ale_py\n",
        "\n",
        "# Import gymnasium to register ALE environments\n",
        "import gymnasium as gym\n",
        "\n",
        "# Verify installation\n",
        "ale_envs = [env_id for env_id in gym.envs.registry.keys() if 'ALE' in env_id]\n",
        "print(f\"‚úÖ All packages installed successfully!\")\n",
        "print(f\"‚úÖ Found {len(ale_envs)} ALE/Atari environments\")\n",
        "\n",
        "if len(ale_envs) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è ROMs not found. Please RESTART THE RUNTIME now!\")\n",
        "    print(\"After restart, skip this cell and run the import cell directly.\")\n",
        "else:\n",
        "    print(\"üéâ You're ready to go! Continue to the next cells.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f54d5c",
      "metadata": {
        "id": "d0f54d5c"
      },
      "source": [
        "**bold text**###  Installing Required Libraries\n",
        "\n",
        "nstalled:**\n",
        "- **gymnasium**: The RL environment framework\n",
        "- **ale-py**: Arcade Learning Environment for Atari games\n",
        "- **AutoROM**: Downloads Atari game ROM files\n",
        "- **opencv-python**: Image processing\n",
        "- **numpy**: Numerical computing\n",
        "- **matplotlib**: Visualization\n",
        "- **tensorflow**: Deep learning framework\n",
        "\n",
        "**Why the restart?** Python needs to reload its registry after ROMs are downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "269d9476",
      "metadata": {
        "id": "269d9476"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50c07ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d50c07ac",
        "outputId": "b3331b93-5455-43db-fc9b-b4b88a41aa90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " SUCCESS! Found 104 ALE/Atari environments\n",
            " Breakout is available: True\n"
          ]
        }
      ],
      "source": [
        "# Verify that ALE environments are available\n",
        "import gymnasium as gym\n",
        "\n",
        "# Check if ALE environments are registered\n",
        "ale_envs = [env_id for env_id in gym.envs.registry.keys() if 'ALE' in env_id]\n",
        "\n",
        "if len(ale_envs) > 0:\n",
        "    print(f\" SUCCESS! Found {len(ale_envs)} ALE/Atari environments\")\n",
        "    print(f\" Breakout is available: {'ALE/Breakout-v5' in ale_envs}\")\n",
        "else:\n",
        "    print(\" ERROR: No ALE environments found!\")\n",
        "    print(\"  Please restart the runtime and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f95836",
      "metadata": {
        "id": "84f95836"
      },
      "source": [
        "###  Importing the Libraries\n",
        "\n",
        "Now we import the libraries we just installed:\n",
        "- **gym**: To create and interact with the Breakout game\n",
        "- **numpy**: For handling arrays and numbers\n",
        "- **cv2**: OpenCV for image processing\n",
        "- **random**: For random action selection during exploration\n",
        "- **deque**: A special list that automatically removes old items\n",
        "- **tensorflow/keras**: For building and training neural networks\n",
        "- **matplotlib**: For displaying game frames and graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3168900a",
      "metadata": {
        "id": "3168900a"
      },
      "source": [
        "##  Step 2: Create and Wrap the Breakout Environment\n",
        "\n",
        "**What happens here:**\n",
        "We create the game environment where our agent will learn to play. The environment is like a video game console - it shows us what's on screen, accepts our button presses (actions), and tells us our score (rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "dc40c58d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "dc40c58d",
        "outputId": "46a0f303-c053-497f-bb59-bc2ab686d136"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def make_env():\n",
        "    env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "obs, info = env.reset()\n",
        "plt.imshow(obs)\n",
        "plt.title(\"Raw Breakout Frame\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f941a4",
      "metadata": {
        "id": "d4f941a4"
      },
      "source": [
        "### üéÆ Creating the Breakout Game Environment\n",
        "\n",
        "This code does three things:\n",
        "1. **Creates** the Breakout-v5 game environment\n",
        "2. **Resets** the environment to get the first game screen\n",
        "3. **Displays** the raw game frame so you can see what the agent will see\n",
        "\n",
        "The environment gives us:\n",
        "- **obs**: The game screen (an image with pixels)\n",
        "- **info**: Extra information about the game state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd545488",
      "metadata": {
        "id": "dd545488"
      },
      "source": [
        "## üèóÔ∏è Step 3: Fixed Architectures\n",
        "\n",
        "**What happens here:**\n",
        "We define two neural network architectures that the agent can use to learn. Think of these as the \"brain\" of the agent - they take in game images and decide which action to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "609f540c",
      "metadata": {
        "id": "609f540c"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model(num_actions):\n",
        "    inputs = layers.Input(shape=(84, 84, 4))\n",
        "\n",
        "    x = layers.Conv2D(32, 8, strides=4, activation='relu')(inputs)\n",
        "    x = layers.Conv2D(64, 4, strides=2, activation='relu')(x)\n",
        "    x = layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_actions)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def create_mlp_model(num_actions):\n",
        "    inputs = layers.Input(shape=(84,84,4))\n",
        "\n",
        "    x = layers.Flatten()(inputs)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(num_actions)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def select_architecture(arch_name, num_actions):\n",
        "    if arch_name == \"cnn\":\n",
        "        return create_cnn_model(num_actions)\n",
        "    elif arch_name == \"mlp\":\n",
        "        return create_mlp_model(num_actions)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid architecture selection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d637d766",
      "metadata": {
        "id": "d637d766"
      },
      "source": [
        "###  Neural Network Architectures\n",
        "\n",
        "**Two architectures are available:**\n",
        "\n",
        "1. **CNN (Convolutional Neural Network)** - Recommended for image-based games\n",
        "   - Uses 3 convolutional layers to detect visual patterns (edges, shapes, objects)\n",
        "   - Has 1 fully connected layer with 512 neurons\n",
        "   - Best for games like Breakout where the agent sees pixels\n",
        "   \n",
        "2. **MLP (Multi-Layer Perceptron)** - Simple fully connected network\n",
        "   - Uses 2 hidden layers with 256 neurons each\n",
        "   - Simpler but less effective for images\n",
        "   - Can be used for comparison\n",
        "\n",
        "**Important**: These architectures are fixed and cannot be changed. You can only choose which one to use via the hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87283d8",
      "metadata": {
        "id": "b87283d8"
      },
      "source": [
        "##  Step 4: Hyperparameter Block\n",
        "### **ONLY MODIFY THE VALUES IN THIS SECTION**\n",
        "\n",
        "**What happens here:**\n",
        "This is the ONLY section you can modify! Here you set the learning parameters that control how your agent learns. Each student should experiment with different values to see what works best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414c78cf",
      "metadata": {
        "id": "414c78cf"
      },
      "outputs": [],
      "source": [
        "HYPERPARAMS = {\n",
        "    \"architecture\": \"cnn\",   # cnn or mlp\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"gamma\": 0.99,\n",
        "    \"epsilon_start\": 1.0,\n",
        "    \"epsilon_end\": 0.1,\n",
        "    \"epsilon_decay_frames\": 200000,\n",
        "    \"batch_size\": 32,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"train_start\": 10000,\n",
        "    \"target_update_freq\": 5000,\n",
        "    \"max_episodes\": 3000\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fec867c",
      "metadata": {
        "id": "0fec867c"
      },
      "source": [
        "### üéØ Configuring Your Hyperparameters\n",
        "\n",
        "**What each hyperparameter means:**\n",
        "\n",
        "- **architecture**: Choose \"cnn\" or \"mlp\" - which brain design to use\n",
        "- **learning_rate**: How fast the agent learns (0.00025 = slow but stable)\n",
        "- **gamma**: How much to value future rewards (0.99 = very forward-thinking)\n",
        "- **epsilon_start**: Starting exploration rate (1.0 = 100% random actions at first)\n",
        "- **epsilon_end**: Final exploration rate (0.1 = always keep 10% randomness)\n",
        "- **epsilon_decay_frames**: How many steps to reduce exploration gradually\n",
        "- **batch_size**: How many experiences to learn from at once\n",
        "- **buffer_size**: How many past experiences to remember\n",
        "- **train_start**: Wait this many steps before training begins (collect data first)\n",
        "- **target_update_freq**: How often to update the stable target network\n",
        "- **max_episodes**: How many complete games to play during training\n",
        "\n",
        "**Try different values and compare results with your team!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a795075c",
      "metadata": {
        "id": "a795075c"
      },
      "source": [
        "## üßπ Step 5: Observation Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24920721",
      "metadata": {
        "id": "24920721"
      },
      "outputs": [],
      "source": [
        "def preprocess(frame):\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame_resized = cv2.resize(frame_gray, (84,84))\n",
        "    return frame_resized / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d0c4155",
      "metadata": {
        "id": "2d0c4155"
      },
      "source": [
        "### üñºÔ∏è Image Processing Function\n",
        "\n",
        "This function does three things to each game frame:\n",
        "1. **Convert to grayscale**: Remove color information (we only need shapes, not colors)\n",
        "2. **Resize to 84x84**: Make the image smaller for faster processing\n",
        "3. **Normalize**: Scale pixel values from 0-255 to 0-1 (helps the neural network learn better)\n",
        "\n",
        "Think of it like taking a photo and making it smaller and black-and-white to save space!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c011279a",
      "metadata": {
        "id": "c011279a"
      },
      "source": [
        "## üì¶ Step 6: Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6402f79",
      "metadata": {
        "id": "c6402f79"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ee759f",
      "metadata": {
        "id": "68ee759f"
      },
      "source": [
        "### üíæ Replay Buffer Class\n",
        "\n",
        "This class creates a storage system for experiences:\n",
        "- **add()**: Saves a new experience (state, action, reward, next_state, done)\n",
        "- **sample()**: Randomly picks a batch of experiences to learn from\n",
        "\n",
        "**Why use a replay buffer?**\n",
        "- Breaks correlation between consecutive experiences\n",
        "- Allows the agent to learn from past successes and failures\n",
        "- Makes training more stable and efficient\n",
        "- Reuses experiences multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db488a55",
      "metadata": {
        "id": "db488a55"
      },
      "source": [
        "## ü§ñ Step 7: Build the Selected Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfef8843",
      "metadata": {
        "id": "bfef8843"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n\n",
        "model = select_architecture(HYPERPARAMS[\"architecture\"], num_actions)\n",
        "target_model = select_architecture(HYPERPARAMS[\"architecture\"], num_actions)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=HYPERPARAMS[\"learning_rate\"])\n",
        "loss_fn = keras.losses.Huber()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d8c696",
      "metadata": {
        "id": "75d8c696"
      },
      "source": [
        "### üîß Creating the Q-Networks\n",
        "\n",
        "This code sets up:\n",
        "1. **Main Model**: The neural network that learns and gets updated frequently\n",
        "2. **Target Model**: A stable copy used for calculating target Q-values\n",
        "3. **Optimizer**: Adam optimizer that adjusts the network weights during training\n",
        "4. **Loss Function**: Huber loss that measures how wrong the predictions are\n",
        "\n",
        "**Why two models?**\n",
        "Using a target model that updates slowly prevents the training from becoming unstable. It's like having a steady reference point while learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063f25ee",
      "metadata": {
        "id": "063f25ee"
      },
      "source": [
        "## üèãÔ∏è Step 8: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f089c6",
      "metadata": {
        "id": "f7f089c6"
      },
      "outputs": [],
      "source": [
        "buffer = ReplayBuffer(HYPERPARAMS[\"buffer_size\"])\n",
        "\n",
        "epsilon = HYPERPARAMS[\"epsilon_start\"]\n",
        "frame_count = 0\n",
        "\n",
        "for episode in range(HYPERPARAMS[\"max_episodes\"]):\n",
        "    obs, _ = env.reset()\n",
        "    state = preprocess(obs)\n",
        "    state_stack = np.stack([state]*4, axis=-1)\n",
        "\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        frame_count += 1\n",
        "\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            q_vals = model(np.expand_dims(state_stack, axis=0))\n",
        "            action = np.argmax(q_vals[0].numpy())\n",
        "\n",
        "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        next_state = preprocess(next_obs)\n",
        "        next_state_stack = np.append(state_stack[:,:,1:], np.expand_dims(next_state, axis=-1), axis=-1)\n",
        "\n",
        "        buffer.add((state_stack, action, reward, next_state_stack, done))\n",
        "        state_stack = next_state_stack\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if frame_count > HYPERPARAMS[\"train_start\"]:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(HYPERPARAMS[\"batch_size\"])\n",
        "\n",
        "            next_q = target_model(next_states)\n",
        "            max_next_q = np.max(next_q.numpy(), axis=1)\n",
        "            target_q = rewards + (1 - dones) * HYPERPARAMS[\"gamma\"] * max_next_q\n",
        "\n",
        "            masks = tf.one_hot(actions, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                q_values = model(states)\n",
        "                q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
        "                loss = loss_fn(target_q, q_action)\n",
        "\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        epsilon = max(HYPERPARAMS[\"epsilon_end\"], epsilon - (HYPERPARAMS[\"epsilon_start\"] - HYPERPARAMS[\"epsilon_end\"]) / HYPERPARAMS[\"epsilon_decay_frames\"])\n",
        "\n",
        "        if frame_count % HYPERPARAMS[\"target_update_freq\"] == 0:\n",
        "            target_model.set_weights(model.get_weights())\n",
        "\n",
        "    print(f\"Episode {episode} ‚Äî Reward: {episode_reward}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7a79dc",
      "metadata": {
        "id": "db7a79dc"
      },
      "source": [
        "### üéÆ The Main Training Process\n",
        "\n",
        "**What this code does step-by-step:**\n",
        "\n",
        "1. **Initialize replay buffer and exploration rate (epsilon)**\n",
        "2. **For each episode (game):**\n",
        "   - Reset the environment and get starting state\n",
        "   - Stack 4 frames together (to detect motion)\n",
        "   \n",
        "3. **For each step in the episode:**\n",
        "   - **Choose action**: Either random (explore) or best action (exploit)\n",
        "   - **Take action**: Execute it in the game\n",
        "   - **Store experience**: Save (state, action, reward, next_state) in buffer\n",
        "   - **Learn from experience**: Sample random batch from buffer and train\n",
        "   - **Update networks**: Adjust weights using gradient descent\n",
        "   - **Decay epsilon**: Gradually reduce exploration over time\n",
        "   - **Update target network**: Copy main model weights periodically\n",
        "\n",
        "4. **Print progress**: Show reward for each episode\n",
        "\n",
        "**Key concepts:**\n",
        "- **Epsilon-greedy**: Balance between exploring (random) and exploiting (learned)\n",
        "- **Experience replay**: Learn from random samples, not just recent experiences\n",
        "- **Target network**: Stable reference prevents training instability\n",
        "- **Frame stacking**: 4 frames together help detect ball movement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb34545",
      "metadata": {
        "id": "bcb34545"
      },
      "source": [
        "## üíæ Step 9: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e224e524",
      "metadata": {
        "id": "e224e524"
      },
      "outputs": [],
      "source": [
        "model.save(\"breakout_dqn_model.h5\")\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2f1b090",
      "metadata": {
        "id": "d2f1b090"
      },
      "source": [
        "### üíø Saving Your Trained Agent\n",
        "\n",
        "This saves all the neural network weights (the \"brain\") to a file called `breakout_dqn_model.h5`.\n",
        "\n",
        "**Why save the model?**\n",
        "- You can load it later without retraining (saves hours!)\n",
        "- You can share it with others\n",
        "- You can use it in other scripts to watch the agent play\n",
        "- You can continue training from this checkpoint if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c62034",
      "metadata": {
        "id": "25c62034"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure();\n",
        "try:\n",
        "    plt.plot(losses); plt.title('Training Loss'); plt.show()\n",
        "except: print('losses not defined')\n",
        "plt.figure();\n",
        "try:\n",
        "    plt.plot(rewards); plt.title('Rewards'); plt.show()\n",
        "except: print('rewards not defined')\n",
        "plt.figure();\n",
        "try:\n",
        "    plt.plot(q_values); plt.title('Q-values'); plt.show()\n",
        "except: print('q_values not defined')\n",
        "plt.figure();\n",
        "try:\n",
        "    plt.plot(epsilons); plt.title('Epsilon Decay'); plt.show()\n",
        "except: print('epsilons not defined')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e53587cf",
      "metadata": {
        "id": "e53587cf"
      },
      "source": [
        "## üìä Step 10: Visualize Training Results (Optional)\n",
        "\n",
        "**What happens here:**\n",
        "This cell attempts to plot training metrics if they were tracked during training. Currently, these variables aren't being saved during training, so the plots won't appear.\n",
        "\n",
        "**To make this work, you would need to:**\n",
        "- Track losses, rewards, q_values, and epsilon values during training\n",
        "- Store them in lists as training progresses\n",
        "- Then these plots will show your agent's learning progress over time"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
