{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "542a382d",
      "metadata": {
        "id": "542a382d"
      },
      "source": [
        "# Training a DQN Agent for Breakout-v5\n",
        "This notebook provides a clean and modular training pipeline for Deep Q-Learning using **Breakout-v5**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aacc912",
      "metadata": {
        "id": "3aacc912"
      },
      "source": [
        "##  Step 1: Install and Import Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a36e0bbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a36e0bbe",
        "outputId": "47fc99d9-1650-4a90-944b-d65d10f93b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "All packages installed successfully!\n",
            "Found 104 ALE/Atari environments\n",
            "You're ready to go! Continue to the next cells.\n"
          ]
        }
      ],
      "source": [
        "# Install all required packages\n",
        "!pip install -q gymnasium ale-py opencv-python numpy matplotlib tensorflow\n",
        "\n",
        "# Install AutoROM and download ROMs\n",
        "!pip install -q autorom\n",
        "!AutoROM --accept-license\n",
        "\n",
        "# Import ale_py to automatically register ROMs\n",
        "import ale_py\n",
        "\n",
        "# Import gymnasium to register ALE environments\n",
        "import gymnasium as gym\n",
        "\n",
        "# Verify installation\n",
        "ale_envs = [env_id for env_id in gym.envs.registry.keys() if 'ALE' in env_id]\n",
        "print(f\"All packages installed successfully!\")\n",
        "print(f\"Found {len(ale_envs)} ALE/Atari environments\")\n",
        "\n",
        "if len(ale_envs) == 0:\n",
        "    print(\"\\n ROMs not found. Please RESTART THE RUNTIME now!\")\n",
        "    print(\"After restart, skip this cell and run the import cell directly.\")\n",
        "else:\n",
        "    print(\"You're ready to go! Continue to the next cells.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f54d5c",
      "metadata": {
        "id": "d0f54d5c"
      },
      "source": [
        "## Installing Required Libraries\n",
        "\n",
        "- **gymnasium**: The RL environment framework\n",
        "- **ale-py**: Arcade Learning Environment for Atari games\n",
        "- **AutoROM**: Downloads Atari game ROM files\n",
        "- **opencv-python**: Image processing\n",
        "- **numpy**: Numerical computing\n",
        "- **matplotlib**: Visualization\n",
        "- **tensorflow**: Deep learning framework\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "269d9476",
      "metadata": {
        "id": "269d9476"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d50c07ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d50c07ac",
        "outputId": "8edb8e2f-0575-44ae-f21c-b9a34accddbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " SUCCESS! Found 104 ALE/Atari environments\n",
            " Breakout is available: True\n"
          ]
        }
      ],
      "source": [
        "# Verify that ALE environments are available\n",
        "import gymnasium as gym\n",
        "\n",
        "# Check if ALE environments are registered\n",
        "ale_envs = [env_id for env_id in gym.envs.registry.keys() if 'ALE' in env_id]\n",
        "\n",
        "if len(ale_envs) > 0:\n",
        "    print(f\" SUCCESS! Found {len(ale_envs)} ALE/Atari environments\")\n",
        "    print(f\" Breakout is available: {'ALE/Breakout-v5' in ale_envs}\")\n",
        "else:\n",
        "    print(\" ERROR: No ALE environments found!\")\n",
        "    print(\"  Please restart the runtime and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f95836",
      "metadata": {
        "id": "84f95836"
      },
      "source": [
        "###  Importing the Libraries\n",
        "\n",
        "Now we import the libraries we just installed:\n",
        "- **gym**: To create and interact with the Breakout game\n",
        "- **numpy**: For handling arrays and numbers\n",
        "- **cv2**: OpenCV for image processing\n",
        "- **random**: For random action selection during exploration\n",
        "- **deque**: A special list that automatically removes old items\n",
        "- **tensorflow/keras**: For building and training neural networks\n",
        "- **matplotlib**: For displaying game frames and graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3168900a",
      "metadata": {
        "id": "3168900a"
      },
      "source": [
        "##  Step 2: Create and Wrap the Breakout Environment\n",
        "\n",
        "**What happens here:**\n",
        "We create the game environment where our agent will learn to play. The environment is like a video game console - it shows us what's on screen, accepts our button presses (actions), and tells us our score (rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dc40c58d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "dc40c58d",
        "outputId": "91342d34-2a8c-4cd1-c4fc-df691d9a421c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def make_env():\n",
        "    env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "obs, info = env.reset()\n",
        "plt.imshow(obs)\n",
        "plt.title(\"Raw Breakout Frame\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4f941a4",
      "metadata": {
        "id": "d4f941a4"
      },
      "source": [
        "### ðŸŽ® Creating the Breakout Game Environment\n",
        "\n",
        "This code does three things:\n",
        "1. **Creates** the Breakout-v5 game environment\n",
        "2. **Resets** the environment to get the first game screen\n",
        "3. **Displays** the raw game frame so you can see what the agent will see\n",
        "\n",
        "The environment gives us:\n",
        "- **obs**: The game screen (an image with pixels)\n",
        "- **info**: Extra information about the game state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd545488",
      "metadata": {
        "id": "dd545488"
      },
      "source": [
        "## Step 3: Fixed Architectures\n",
        "\n",
        "**What happens here:**\n",
        "We define two neural network architectures that the agent can use to learn. Think of these as the \"brain\" of the agent - they take in game images and decide which action to take."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "609f540c",
      "metadata": {
        "id": "609f540c"
      },
      "outputs": [],
      "source": [
        "def create_cnn_model(num_actions):\n",
        "    inputs = layers.Input(shape=(84, 84, 4))\n",
        "\n",
        "    x = layers.Conv2D(32, 8, strides=4, activation='relu')(inputs)\n",
        "    x = layers.Conv2D(64, 4, strides=2, activation='relu')(x)\n",
        "    x = layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_actions)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def create_mlp_model(num_actions):\n",
        "    inputs = layers.Input(shape=(84,84,4))\n",
        "\n",
        "    x = layers.Flatten()(inputs)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = layers.Dense(256, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(num_actions)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "def select_architecture(arch_name, num_actions):\n",
        "    if arch_name == \"cnn\":\n",
        "        return create_cnn_model(num_actions)\n",
        "    elif arch_name == \"mlp\":\n",
        "        return create_mlp_model(num_actions)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid architecture selection\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d637d766",
      "metadata": {
        "id": "d637d766"
      },
      "source": [
        "###  Neural Network Architectures\n",
        "\n",
        "**Two architectures are available:**\n",
        "\n",
        "1. **CNN (Convolutional Neural Network)** - Recommended for image-based games\n",
        "   - Uses 3 convolutional layers to detect visual patterns (edges, shapes, objects)\n",
        "   - Has 1 fully connected layer with 512 neurons\n",
        "   - Best for games like Breakout where the agent sees pixels\n",
        "   \n",
        "2. **MLP (Multi-Layer Perceptron)** - Simple fully connected network\n",
        "   - Uses 2 hidden layers with 256 neurons each\n",
        "   - Simpler but less effective for images\n",
        "   - Can be used for comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87283d8",
      "metadata": {
        "id": "b87283d8"
      },
      "source": [
        "##  Step 4: Hyperparameter Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "414c78cf",
      "metadata": {
        "id": "414c78cf"
      },
      "outputs": [],
      "source": [
        "HYPERPARAMS= {\n",
        "    \"architecture\": \"mlp\",\n",
        "    \"learning_rate\": 0.0004,\n",
        "    \"gamma\": 0.95,\n",
        "    \"epsilon_start\": 1.0,\n",
        "    \"epsilon_end\": 0.08,\n",
        "    \"epsilon_decay_frames\": 40000,\n",
        "    \"batch_size\": 12,\n",
        "    \"buffer_size\": 6000,\n",
        "    \"train_start\": 800,\n",
        "    \"target_update_freq\": 1200,\n",
        "    \"max_episodes\": 200\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fec867c",
      "metadata": {
        "id": "0fec867c"
      },
      "source": [
        "### ðŸŽ¯ Configuring Your Hyperparameters\n",
        "\n",
        "**What each hyperparameter means:**\n",
        "\n",
        "- **architecture**: Choose \"cnn\" or \"mlp\" - which brain design to use\n",
        "- **learning_rate**: How fast the agent learns (0.00025 = slow but stable)\n",
        "- **gamma**: How much to value future rewards (0.99 = very forward-thinking)\n",
        "- **epsilon_start**: Starting exploration rate (1.0 = 100% random actions at first)\n",
        "- **epsilon_end**: Final exploration rate (0.1 = always keep 10% randomness)\n",
        "- **epsilon_decay_frames**: How many steps to reduce exploration gradually\n",
        "- **batch_size**: How many experiences to learn from at once\n",
        "- **buffer_size**: How many past experiences to remember\n",
        "- **train_start**: Wait this many steps before training begins (collect data first)\n",
        "- **target_update_freq**: How often to update the stable target network\n",
        "- **max_episodes**: How many complete games to play during training\n",
        "\n",
        "**Try different values and compare results with your team!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a795075c",
      "metadata": {
        "id": "a795075c"
      },
      "source": [
        "## Step 5: Observation Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "24920721",
      "metadata": {
        "id": "24920721"
      },
      "outputs": [],
      "source": [
        "def preprocess(frame):\n",
        "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame_resized = cv2.resize(frame_gray, (84,84))\n",
        "    return frame_resized / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d0c4155",
      "metadata": {
        "id": "2d0c4155"
      },
      "source": [
        "### Image Processing Function\n",
        "\n",
        "This function does three things to each game frame:\n",
        "1. **Convert to grayscale**: Remove color information (we only need shapes, not colors)\n",
        "2. **Resize to 84x84**: Make the image smaller for faster processing\n",
        "3. **Normalize**: Scale pixel values from 0-255 to 0-1 (helps the neural network learn better)\n",
        "\n",
        "Think of it like taking a photo and making it smaller and black-and-white to save space!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c011279a",
      "metadata": {
        "id": "c011279a"
      },
      "source": [
        "##  Step 6: Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c6402f79",
      "metadata": {
        "id": "c6402f79"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        state, action, reward, next_state, done = experience\n",
        "\n",
        "        # Convert to uint8 (VERY IMPORTANT)\n",
        "        state = state.astype(np.uint8)\n",
        "        next_state = next_state.astype(np.uint8)\n",
        "\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*[self.buffer[i] for i in indices])\n",
        "\n",
        "        # Convert back to float32 for training and normalize\n",
        "        states = np.array(states, dtype=np.float32) / 255.0\n",
        "        next_states = np.array(next_states, dtype=np.float32) / 255.0\n",
        "\n",
        "        return (\n",
        "            states,\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            next_states,\n",
        "            np.array(dones)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ee759f",
      "metadata": {
        "id": "68ee759f"
      },
      "source": [
        " Replay Buffer Class\n",
        "\n",
        "This class creates a storage system for experiences:\n",
        "- **add()**: Saves a new experience (state, action, reward, next_state, done)\n",
        "- **sample()**: Randomly picks a batch of experiences to learn from\n",
        "\n",
        "**Why use a replay buffer?**\n",
        "- Breaks correlation between consecutive experiences\n",
        "- Allows the agent to learn from past successes and failures\n",
        "- Makes training more stable and efficient\n",
        "- Reuses experiences multiple times"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db488a55",
      "metadata": {
        "id": "db488a55"
      },
      "source": [
        "## ðŸ¤– Step 7: Build the Selected Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bfef8843",
      "metadata": {
        "id": "bfef8843"
      },
      "outputs": [],
      "source": [
        "num_actions = env.action_space.n\n",
        "model = select_architecture(HYPERPARAMS[\"architecture\"], num_actions)\n",
        "target_model = select_architecture(HYPERPARAMS[\"architecture\"], num_actions)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=HYPERPARAMS[\"learning_rate\"])\n",
        "loss_fn = keras.losses.Huber()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d8c696",
      "metadata": {
        "id": "75d8c696"
      },
      "source": [
        "### ðŸ”§ Creating the Q-Networks\n",
        "\n",
        "This code sets up:\n",
        "1. **Main Model**: The neural network that learns and gets updated frequently\n",
        "2. **Target Model**: A stable copy used for calculating target Q-values\n",
        "3. **Optimizer**: Adam optimizer that adjusts the network weights during training\n",
        "4. **Loss Function**: Huber loss that measures how wrong the predictions are\n",
        "\n",
        "**Why two models?**\n",
        "Using a target model that updates slowly prevents the training from becoming unstable. It's like having a steady reference point while learning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(model, env, num_eval_episodes=10, epsilon=0.05):\n",
        "    \"\"\"\n",
        "    Evaluate agent performance with low exploration\n",
        "\n",
        "    Args:\n",
        "        model: Trained Q-network\n",
        "        env: Gym environment\n",
        "        num_eval_episodes: Number of episodes to average\n",
        "        epsilon: Low exploration rate for evaluation\n",
        "\n",
        "    Returns:\n",
        "        Average reward over evaluation episodes\n",
        "    \"\"\"\n",
        "    eval_rewards = []\n",
        "\n",
        "    for _ in range(num_eval_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        state = preprocess(obs)\n",
        "        state_stack = np.stack([state]*4, axis=-1)\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Mostly exploit, minimal exploration\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_vals = model(np.expand_dims(state_stack, axis=0))\n",
        "                action = np.argmax(q_vals[0].numpy())\n",
        "\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            next_state = preprocess(next_obs)\n",
        "            state_stack = np.append(state_stack[:,:,1:],\n",
        "                                   np.expand_dims(next_state, axis=-1),\n",
        "                                   axis=-1)\n",
        "            episode_reward += reward\n",
        "\n",
        "        eval_rewards.append(episode_reward)\n",
        "\n",
        "    return np.mean(eval_rewards)"
      ],
      "metadata": {
        "id": "15zUr2DF4ALp"
      },
      "id": "15zUr2DF4ALp",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "063f25ee",
      "metadata": {
        "id": "063f25ee"
      },
      "source": [
        "## Step 8: Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f7f089c6",
      "metadata": {
        "id": "f7f089c6"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import psutil\n",
        "def train_dqn_agent(env, model, target_model, optimizer, loss_fn,\n",
        "                   hyperparams, model_name=\"breakout_model\"):\n",
        "    \"\"\"\n",
        "    Train a DQN agent with evaluation-based model saving\n",
        "    \"\"\"\n",
        "    buffer = ReplayBuffer(hyperparams[\"buffer_size\"])\n",
        "    epsilon = hyperparams[\"epsilon_start\"]\n",
        "    frame_count = 0\n",
        "\n",
        "    # Tracking\n",
        "    episode_rewards = []\n",
        "    episode_losses = []\n",
        "    episode_epsilons = []\n",
        "    episode_steps = []\n",
        "\n",
        "    # IMPROVED: Evaluation-based best model tracking\n",
        "    best_eval_reward = -float('inf')\n",
        "    best_model_weights = None\n",
        "    best_episode = 0\n",
        "    eval_history = []\n",
        "    eval_episodes_list = []\n",
        "\n",
        "    # Evaluation settings\n",
        "    EVAL_FREQUENCY = 50\n",
        "    NUM_EVAL_EPISODES = 5  # Reduced for faster evaluation\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    print(f\"Starting training for {model_name}...\")\n",
        "    print(f\"Total episodes: {hyperparams['max_episodes']}\")\n",
        "    print(f\"Evaluation every {EVAL_FREQUENCY} episodes\")\n",
        "    print(f\"Training will start after {hyperparams['train_start']} frames\\n\")\n",
        "\n",
        "    for episode in range(hyperparams[\"max_episodes\"]):\n",
        "        obs, _ = env.reset()\n",
        "        state = preprocess(obs)\n",
        "        state_stack = np.stack([state]*4, axis=-1)\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_loss_sum = 0\n",
        "        loss_count = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "\n",
        "        # Training episode\n",
        "        while not done:\n",
        "            frame_count += 1\n",
        "            steps += 1\n",
        "\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                q_vals = model(np.expand_dims(state_stack, axis=0))\n",
        "                action = np.argmax(q_vals[0].numpy())\n",
        "\n",
        "            frame_skip = 2\n",
        "            total_reward = 0\n",
        "            for _ in range(frame_skip):\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                total_reward += reward\n",
        "                if terminated or truncated:\n",
        "                    done = True\n",
        "                    break\n",
        "\n",
        "            next_state = preprocess(next_obs)\n",
        "            next_state_stack = np.append(state_stack[:,:,1:],\n",
        "                                        np.expand_dims(next_state, axis=-1),\n",
        "                                        axis=-1)\n",
        "\n",
        "            buffer.add((state_stack, action, total_reward, next_state_stack, done))\n",
        "            state_stack = next_state_stack\n",
        "            episode_reward += total_reward\n",
        "\n",
        "            # Training step\n",
        "            if frame_count > hyperparams[\"train_start\"]:\n",
        "                states, actions, rewards, next_states, dones = buffer.sample(\n",
        "                    hyperparams[\"batch_size\"])\n",
        "\n",
        "                next_q = target_model(next_states)\n",
        "                max_next_q = np.max(next_q.numpy(), axis=1)\n",
        "                target_q = rewards + (1 - dones) * hyperparams[\"gamma\"] * max_next_q\n",
        "\n",
        "                masks = tf.one_hot(actions, num_actions)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    q_values = model(states)\n",
        "                    q_action = tf.reduce_sum(q_values * masks, axis=1)\n",
        "                    loss = loss_fn(target_q, q_action)\n",
        "\n",
        "                grads = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "                episode_loss_sum += loss.numpy()\n",
        "                loss_count += 1\n",
        "\n",
        "            # Epsilon decay\n",
        "            epsilon = max(hyperparams[\"epsilon_end\"],\n",
        "                         hyperparams[\"epsilon_start\"] -\n",
        "                         (hyperparams[\"epsilon_start\"] - hyperparams[\"epsilon_end\"]) *\n",
        "                         (frame_count / hyperparams[\"epsilon_decay_frames\"]))\n",
        "\n",
        "            # Target network update\n",
        "            if frame_count % hyperparams[\"target_update_freq\"] == 0:\n",
        "                target_model.set_weights(model.get_weights())\n",
        "\n",
        "        # Store metrics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_epsilons.append(epsilon)\n",
        "        episode_steps.append(steps)\n",
        "        avg_loss = episode_loss_sum / loss_count if loss_count > 0 else 0\n",
        "        episode_losses.append(avg_loss)\n",
        "\n",
        "        # ========================================\n",
        "        # IMPROVED: EVALUATION-BASED MODEL SAVING\n",
        "        # ========================================\n",
        "        if (episode + 1) % EVAL_FREQUENCY == 0:\n",
        "            print(f\"Evaluating agent at episode {episode+1}...\")\n",
        "\n",
        "            eval_reward = evaluate_agent(model, env,\n",
        "                                        num_eval_episodes=NUM_EVAL_EPISODES,\n",
        "                                        epsilon=0.05)\n",
        "\n",
        "            eval_history.append(eval_reward)\n",
        "            eval_episodes_list.append(episode + 1)\n",
        "\n",
        "            print(f\"Evaluation Reward: {eval_reward:.2f}\")\n",
        "\n",
        "            # Save if this is the best evaluation performance\n",
        "            if eval_reward > best_eval_reward:\n",
        "                best_eval_reward = eval_reward\n",
        "                best_model_weights = model.get_weights()\n",
        "                best_episode = episode\n",
        "\n",
        "                # Save immediately\n",
        "                model.save(f\"{model_name}_best.keras\")\n",
        "                print(f\"NEW BEST MODEL! Eval Reward: {eval_reward:.2f} - SAVED!\")\n",
        "                print(f\"   (Previous best: {best_eval_reward:.2f})\")\n",
        "            else:\n",
        "                print(f\"   (Current best: {best_eval_reward:.2f} at episode {best_episode+1})\")\n",
        "            print()\n",
        "\n",
        "        # Memory management\n",
        "        if episode % 50 == 0:\n",
        "            gc.collect()\n",
        "            memory_usage = psutil.virtual_memory().percent\n",
        "            if memory_usage > 85:\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Memory cleaning at {memory_usage}%\")\n",
        "\n",
        "        # Progress printing\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode+1}/{hyperparams['max_episodes']} | \"\n",
        "                  f\"Reward: {episode_reward:.1f} | \"\n",
        "                  f\"Avg(10): {avg_reward:.1f} | \"\n",
        "                  f\"Epsilon: {epsilon:.3f} | \"\n",
        "                  f\"Steps: {steps} | \"\n",
        "                  f\"Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "    # Final model saving\n",
        "    if best_model_weights is not None:\n",
        "        model.set_weights(best_model_weights)\n",
        "        model.save(f\"{model_name}_best_final.keras\")\n",
        "        print(f\"BEST model saved from episode {best_episode+1}\")\n",
        "        print(f\"Best evaluation reward: {best_eval_reward:.2f}\")\n",
        "\n",
        "    # Save final model for comparison\n",
        "    model.save(f\"{model_name}_final.keras\")\n",
        "    print(f\"Final model also saved\")\n",
        "\n",
        "    # Cleanup\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    return {\n",
        "        'episode_rewards': episode_rewards,\n",
        "        'episode_losses': episode_losses,\n",
        "        'episode_epsilons': episode_epsilons,\n",
        "        'episode_steps': episode_steps,\n",
        "        'eval_history': eval_history,\n",
        "        'eval_episodes': eval_episodes_list,\n",
        "        'best_episode': best_episode,\n",
        "        'best_eval_reward': best_eval_reward\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_history1 = train_dqn_agent(\n",
        "    env=env,\n",
        "    model=model,\n",
        "    target_model=target_model,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    hyperparams=HYPERPARAMS,\n",
        "    model_name=\"mlp_model_fast\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nymP5ik5fM9q",
        "outputId": "a37821a9-7ff3-497c-cc4d-32ff98932848"
      },
      "id": "nymP5ik5fM9q",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for mlp_model_fast...\n",
            "Total episodes: 200\n",
            "Evaluation every 50 episodes\n",
            "Training will start after 800 frames\n",
            "\n",
            "Episode 10/200 | Reward: 1.0 | Avg(10): 2.2 | Epsilon: 0.973 | Steps: 87 | Loss: 0.0084\n",
            "Episode 20/200 | Reward: 3.0 | Avg(10): 2.6 | Epsilon: 0.943 | Steps: 131 | Loss: 0.0089\n",
            "Episode 30/200 | Reward: 0.0 | Avg(10): 1.6 | Epsilon: 0.919 | Steps: 74 | Loss: 0.0061\n",
            "Episode 40/200 | Reward: 3.0 | Avg(10): 1.7 | Epsilon: 0.894 | Steps: 127 | Loss: 0.0087\n",
            "Evaluating agent at episode 50...\n",
            "Evaluation Reward: 1.40\n",
            "NEW BEST MODEL! Eval Reward: 1.40 - SAVED!\n",
            "   (Previous best: 1.40)\n",
            "\n",
            "Episode 50/200 | Reward: 1.0 | Avg(10): 0.9 | Epsilon: 0.873 | Steps: 87 | Loss: 0.0071\n",
            "Episode 60/200 | Reward: 1.0 | Avg(10): 2.1 | Epsilon: 0.845 | Steps: 79 | Loss: 0.0099\n",
            "Episode 70/200 | Reward: 2.0 | Avg(10): 1.9 | Epsilon: 0.817 | Steps: 108 | Loss: 0.0073\n",
            "Episode 80/200 | Reward: 2.0 | Avg(10): 1.8 | Epsilon: 0.790 | Steps: 110 | Loss: 0.0068\n",
            "Episode 90/200 | Reward: 0.0 | Avg(10): 1.9 | Epsilon: 0.763 | Steps: 76 | Loss: 0.0061\n",
            "Evaluating agent at episode 100...\n",
            "Evaluation Reward: 1.60\n",
            "NEW BEST MODEL! Eval Reward: 1.60 - SAVED!\n",
            "   (Previous best: 1.60)\n",
            "\n",
            "Episode 100/200 | Reward: 2.0 | Avg(10): 2.4 | Epsilon: 0.733 | Steps: 142 | Loss: 0.0091\n",
            "Episode 110/200 | Reward: 0.0 | Avg(10): 2.0 | Epsilon: 0.705 | Steps: 74 | Loss: 0.0085\n",
            "Episode 120/200 | Reward: 2.0 | Avg(10): 1.4 | Epsilon: 0.680 | Steps: 109 | Loss: 0.0081\n",
            "Episode 130/200 | Reward: 2.0 | Avg(10): 2.3 | Epsilon: 0.648 | Steps: 135 | Loss: 0.0096\n",
            "Episode 140/200 | Reward: 2.0 | Avg(10): 1.2 | Epsilon: 0.623 | Steps: 116 | Loss: 0.0076\n",
            "Evaluating agent at episode 150...\n",
            "Evaluation Reward: 1.40\n",
            "   (Current best: 1.60 at episode 100)\n",
            "\n",
            "Episode 150/200 | Reward: 3.0 | Avg(10): 2.6 | Epsilon: 0.590 | Steps: 141 | Loss: 0.0061\n",
            "Episode 160/200 | Reward: 4.0 | Avg(10): 1.6 | Epsilon: 0.562 | Steps: 165 | Loss: 0.0057\n",
            "Episode 170/200 | Reward: 0.0 | Avg(10): 1.9 | Epsilon: 0.532 | Steps: 91 | Loss: 0.0044\n",
            "Episode 180/200 | Reward: 3.0 | Avg(10): 1.9 | Epsilon: 0.500 | Steps: 139 | Loss: 0.0103\n",
            "Episode 190/200 | Reward: 5.0 | Avg(10): 2.3 | Epsilon: 0.466 | Steps: 264 | Loss: 0.0076\n",
            "Evaluating agent at episode 200...\n",
            "Evaluation Reward: 0.60\n",
            "   (Current best: 1.60 at episode 100)\n",
            "\n",
            "Episode 200/200 | Reward: 2.0 | Avg(10): 1.6 | Epsilon: 0.435 | Steps: 147 | Loss: 0.0055\n",
            "\n",
            "Training completed!\n",
            "BEST model saved from episode 100\n",
            "Best evaluation reward: 1.60\n",
            "Final model also saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7a79dc",
      "metadata": {
        "id": "db7a79dc"
      },
      "source": [
        "### ðŸŽ® The Main Training Process\n",
        "\n",
        "**What this code does step-by-step:**\n",
        "\n",
        "1. **Initialize replay buffer and exploration rate (epsilon)**\n",
        "2. **For each episode (game):**\n",
        "   - Reset the environment and get starting state\n",
        "   - Stack 4 frames together (to detect motion)\n",
        "   \n",
        "3. **For each step in the episode:**\n",
        "   - **Choose action**: Either random (explore) or best action (exploit)\n",
        "   - **Take action**: Execute it in the game\n",
        "   - **Store experience**: Save (state, action, reward, next_state) in buffer\n",
        "   - **Learn from experience**: Sample random batch from buffer and train\n",
        "   - **Update networks**: Adjust weights using gradient descent\n",
        "   - **Decay epsilon**: Gradually reduce exploration over time\n",
        "   - **Update target network**: Copy main model weights periodically\n",
        "\n",
        "4. **Print progress**: Show reward for each episode\n",
        "\n",
        "**Key concepts:**\n",
        "- **Epsilon-greedy**: Balance between exploring (random) and exploiting (learned)\n",
        "- **Experience replay**: Learn from random samples, not just recent experiences\n",
        "- **Target network**: Stable reference prevents training instability\n",
        "- **Frame stacking**: 4 frames together help detect ball movement"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}